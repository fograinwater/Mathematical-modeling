# 正则化

### 正则化的作用

为了提高模型的泛化能力，我们需要让我们的模型尽可能简单，所以我们需要控制模型复杂度。其实也就是要实现结构风险最小化。

【结构风险最小化】： 在经验风险最小化的基础上（即训练过程中的误差最小化），尽可能采用简单的模型，以提高模型泛化能力。

所以，正则化（Regulation）其实就是机器学习中用于**控制模型复杂度，防止过拟合**的一种常用技术。



### 正则化的基本原理

正则化就是在原目标函数的基础上添加正则化项

要理解正则化的原理，最重要的就是要理解正则化是如何通过添加正则化项而实现降低模型复杂度的。

【基本思想】：

- 在原目标函数中添加正则化项
- 这个**正则化项与最终求解的模型的参数有关**
- 在目标函数优化的过程中，也会**兼顾优化**正则化项，从而降低模型复杂度。

【举个例子】：

如果原目标函数是损失函数，那么我们会将关于模型参数 $w$ 的正则化项加入到损失函数中。

在模型优化的过程中，我们选择的优化算法（梯度下降等）将努力找到一个权重向量 $w$ ，使原始损失函数最小化，同时最小化正则项。

【正则化后的目标函数】：

那么，我们可以得到正则化后的目标函数：

$ J(w; X, y) = J(w; X, y) + \alpha \Omega(w)$

- $J(w; X,y)$：原目标函数 
- $X、y$ ：训练样本和相应标签
- $w$：模型的权重系数向量（表示模型的参数或权重的向量）
- $\alpha$：控制正则化的强弱



### 正则化的常用方法

#### L1正则化

$ \ell_1: \Omega(w) = \parallel w \parallel _1 = \sum_i \vert w_i \vert $

L1正则化能得到稀疏解从而实现特征选择：使得一部分参数变为零，从而减少模型中不重要的特征的影响。

使用L1范数寻找稀疏解的原因是由于其特殊的形状。它的尖峰恰好处于稀疏点。使用它接触解决方案的表面很可能会在尖端找到接触点，从而形成稀疏的解决方案。

【对于L1正则化的稀疏性解释】

参考：[L1正则化的稀疏性解释 - 纯净天空 (vimsky.com)](https://vimsky.com/article/3852.html#respond)

一旦正则项变小，那么最终求解的模型的参数中就会有更多的参数趋于0，那么模型的复杂度也相应降低了。

#### L2正则化

$ \ell_2: \Omega(w) = \parallel w \parallel _2^2 = \sum_i w_i^2 $

L2正则化能得到平滑解，通过使参数较小来减少模型的复杂度，使得模型对数据的扰动更加稳定。